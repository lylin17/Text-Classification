{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forum Posts Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function for tokenize text\n",
    "def tokenize_text(text): \n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens] #remove whitespace in tokens\n",
    "    return tokens\n",
    "\n",
    "#function for expand contractions\n",
    "def expand_contractions(text, contraction_mapping): # contraction mapping is CONTRACTION_MAP from custom contraction.py file\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), #from CONTRACTION_MAP in nltk\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)             # match all contraction keys in CONTRACTION_MAP\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower()) #get key from dictionary + try lower case \n",
    "        expanded_contraction = first_char+expanded_contraction[1:] #keep first char constant and add expanded contraction \n",
    "                                                                   #from 2nd char onwards\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text) # replace matched contraaction pattern with expanded one\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text) #remove the ' from expanded text\n",
    "    return expanded_text\n",
    "\n",
    "# function for POS tags \n",
    "from nltk.corpus import wordnet as wn\n",
    "def pos_tag_text(text): # convert spacy tags to wordnet tags to use wordnet lemmatizer\n",
    "    def wn_tags(token_pos):\n",
    "        if token_pos == 'ADJ':\n",
    "            return wn.ADJ\n",
    "        if token_pos == 'VERB':\n",
    "            return wn.VERB\n",
    "        if token_pos == 'NOUN':\n",
    "            return wn.NOUN\n",
    "        if token_pos == 'ADV':\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "    text = nlp(text)\n",
    "    tagged_text = [(token.orth_,token.pos_) for token in text]\n",
    "    tagged_text = [(token[0].lower(),wn_tags(token[1])) for token in tagged_text] # convert tags and words to lowercase\n",
    "    return tagged_text\n",
    "\n",
    "# function to lemmatize text based on POS tags \n",
    "wnl = WordNetLemmatizer()\n",
    "   \n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag # return lemmatized word if pos tag present\n",
    "                         else word                               # just return word if pos tag is \"None\"\n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens) #join tokens with \" \" between them\n",
    "    return lemmatized_text\n",
    "\n",
    "# function to remove special character\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation))) #punctuation string from re module\n",
    "    filtered_tokens = [pattern.sub('', token) for token in tokens] #replace matching special character with \" \" \n",
    "    filtered_text = ' '.join(filtered_tokens) #join tokens with \" \" between them\n",
    "    return filtered_text\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = list(set(stopword_list)-set(['not','against','down']))\n",
    "stopword_list = stopword_list + ['would', 'could', 'come', 'go', 'get',\n",
    "                                 'tell', 'listen', 'one', 'two', 'three',\n",
    "                                 'four', 'five', 'six', 'seven', 'eight',\n",
    "                                 'nine', 'zero', 'join', 'find', 'make',\n",
    "                                 'say', 'ask', 'tell', 'see', 'try', 'back',\n",
    "                                 'also','send','iphone','forum','use','mobile','app']\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list] # only extract tokens not in stopword list\n",
    "    filtered_text = ' '.join(filtered_tokens) #join tokens with \" \" between them\n",
    "    return filtered_text\n",
    "\n",
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token): #keep only text character \n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def normalize_corpus(corpus, lemmatize=True, \n",
    "                     only_text_chars=False,\n",
    "                     tokenize=False):\n",
    "    \n",
    "    normalized_corpus = []    \n",
    "    for text in corpus:\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        if lemmatize:\n",
    "            text = lemmatize_text(text)\n",
    "        else:\n",
    "            text = text.lower() #text already lower in pos tagging in lemmatize function\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if only_text_chars:\n",
    "            text = keep_text_characters(text)\n",
    "        \n",
    "        if tokenize:\n",
    "            text = tokenize_text(text) #option to tokenize text if it's not already tokenized\n",
    "            normalized_corpus.append(text)\n",
    "        else:\n",
    "            normalized_corpus.append(text)\n",
    "            \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Driving?</td>\n",
       "      <td>Raex</td>\n",
       "      <td>Dec 28, 2011</td>\n",
       "      <td>I've been wondering about this for a while no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Driving?</td>\n",
       "      <td>jaykewashere</td>\n",
       "      <td>Dec 28, 2011</td>\n",
       "      <td>I didn't know driving not high was possible ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driving?</td>\n",
       "      <td>rastaballer209</td>\n",
       "      <td>Dec 28, 2011</td>\n",
       "      <td>Maybe if your a desent driver.i do and a few ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Driving?</td>\n",
       "      <td>amandaa125</td>\n",
       "      <td>Dec 28, 2011</td>\n",
       "      <td>Well I don't think they could give you a DUI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Driving?</td>\n",
       "      <td>The Nickatina</td>\n",
       "      <td>Dec 28, 2011</td>\n",
       "      <td>as lonq as you dont qo too deep into your tho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      title        username          date  \\\n",
       "0  Driving?            Raex  Dec 28, 2011   \n",
       "1  Driving?    jaykewashere  Dec 28, 2011   \n",
       "2  Driving?  rastaballer209  Dec 28, 2011   \n",
       "3  Driving?      amandaa125  Dec 28, 2011   \n",
       "4  Driving?   The Nickatina  Dec 28, 2011   \n",
       "\n",
       "                                             content  \n",
       "0   I've been wondering about this for a while no...  \n",
       "1   I didn't know driving not high was possible ,...  \n",
       "2   Maybe if your a desent driver.i do and a few ...  \n",
       "3   Well I don't think they could give you a DUI ...  \n",
       "4   as lonq as you dont qo too deep into your tho...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#grasscity.com forum\n",
    "raw = pd.read_csv('grasscity.csv',header=0)\n",
    "\n",
    "#cannabis.com forum\n",
    "# raw1 = pd.read_csv('cannabis1.csv',header=0)\n",
    "# raw2 = pd.read_csv('cannabis2.csv',header=0)\n",
    "# raw = pd.concat([raw1,raw2],axis = 0)\n",
    "# raw = raw.reset_index(drop=True)\n",
    "\n",
    "#marijuana.com forum\n",
    "# raw1 = pd.read_csv('marijuana1.csv',header=0)\n",
    "# raw2 = pd.read_csv('marijuana2.csv',header=0)\n",
    "# raw = pd.concat([raw1,raw2],axis = 0)\n",
    "# raw = raw.reset_index(drop=True)\n",
    "\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     I've been wondering about this for a while no...\n",
       "1     I didn't know driving not high was possible ,...\n",
       "2     Maybe if your a desent driver.i do and a few ...\n",
       "3     Well I don't think they could give you a DUI ...\n",
       "4     as lonq as you dont qo too deep into your tho...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = raw['content']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wonder favorite method relaxation sober drive hour hour music obviously appeal high notice lot member quite often whenever friend need drive somewhere actually designate driver always pretty sure dui applicable influence substance view drive high\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "posts = np.array(dataset)\n",
    "norm_posts = normalize_corpus(posts,\n",
    "                              lemmatize=True,\n",
    "                              only_text_chars=True)  \n",
    "print (norm_posts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for marijuana.com due to encoding issues\n",
    "# dataset = np.array(raw['content'])  \n",
    "# def normalize(corpus):\n",
    "#     normalized_corpus = []    \n",
    "#     for text in corpus:\n",
    "#           text = keep_text_characters(text)\n",
    "#           normalized_corpus.append(text)\n",
    "#     return normalized_corpus\n",
    "# dataset = normalize(dataset)\n",
    "# dataset = pd.DataFrame(dataset)\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned = pd.DataFrame(norm_posts, columns=['processed'])\n",
    "cleaned['original'] = dataset\n",
    "\n",
    "def drive_relevance(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    relevant_tokens = [token for token in tokens if token in ['drive','driver','dui']] \n",
    "    relevance = len(relevant_tokens)   \n",
    "    return relevance\n",
    "\n",
    "cleaned['drive_relevance'] = [drive_relevance(post) for post in cleaned['processed']]\n",
    "cleaned = cleaned.loc[cleaned['drive_relevance']>0,]\n",
    "\n",
    "def weed_relevance(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    relevant_tokens = [token for token in tokens if token in ['stone','stoner','high','zooted','bowl','bake','joint','blaze',\n",
    "                                                              'toke','toked','toking','marijuana','cannabis']] \n",
    "    relevance = len(relevant_tokens)   \n",
    "    return relevance\n",
    "\n",
    "cleaned['weed_relevance'] = [weed_relevance(post) for post in cleaned['processed']]\n",
    "cleaned = cleaned.loc[cleaned['weed_relevance']>0,]\n",
    "\n",
    "cleaned=cleaned.drop_duplicates()\n",
    "\n",
    "#cleaned.to_csv(\"cleaned.csv\",header= True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed</th>\n",
       "      <th>original</th>\n",
       "      <th>drive_relevance</th>\n",
       "      <th>weed_relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wonder favorite method relaxation sober drive ...</td>\n",
       "      <td>I've been wondering about this for a while no...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not know drive not high possible put hole road...</td>\n",
       "      <td>I didn't know driving not high was possible ,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>well not think give dui drive high unlike alco...</td>\n",
       "      <td>Well I don't think they could give you a DUI ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quote name amandaa125 well not think give dui ...</td>\n",
       "      <td>[quote name='\"amandaa125\"']Well I don't think...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pretty funny like spot piss test splash cop ac...</td>\n",
       "      <td>It'd be pretty funny if it were like an on th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           processed  \\\n",
       "0  wonder favorite method relaxation sober drive ...   \n",
       "1  not know drive not high possible put hole road...   \n",
       "3  well not think give dui drive high unlike alco...   \n",
       "5  quote name amandaa125 well not think give dui ...   \n",
       "6  pretty funny like spot piss test splash cop ac...   \n",
       "\n",
       "                                            original  drive_relevance  \\\n",
       "0   I've been wondering about this for a while no...                5   \n",
       "1   I didn't know driving not high was possible ,...                1   \n",
       "3   Well I don't think they could give you a DUI ...                2   \n",
       "5   [quote name='\"amandaa125\"']Well I don't think...                2   \n",
       "6   It'd be pretty funny if it were like an on th...                1   \n",
       "\n",
       "   weed_relevance  \n",
       "0               2  \n",
       "1               1  \n",
       "3               2  \n",
       "5               3  \n",
       "6               1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2265, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
